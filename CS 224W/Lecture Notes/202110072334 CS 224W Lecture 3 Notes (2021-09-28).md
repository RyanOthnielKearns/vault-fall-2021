#CS #CS224W #lecture-notes 
# WL Kernel (continuing from [[202110061645 CS 224W Lecture 2 Notes (2021-09-23)|previous lecture]])
- recall: **graph-level features**
	- define similarity function called a **kernel**
- idea: efficient graph descriptor
	- use neighborhood structure to iteratively enrich node vocabulary
		- generalized version of node degrees, which are the "1-hop neighborhood" of every node
	- good for graph isomorphism testing

## Algorithm: Color Refinement
- initialize every node with a color
- iteratively refine node colors using `HASH` function of color of node & color of all the node's neighbors
	- assigns unique identifier to certain combination of node+neighbors (in a deterministic way)
- run for $K$ steps -- color of node at end summarized the structure of the "$K$-hop neighborhood" of the node
	- each step, collecting information about bigger pieces of the network
- after color refinement, WL kernel counts number of nodes w/ given color into a feature rep. vector, and takes dot product to define similarity score
	- keep history of previous iterations in the vector -- describes how node "evolves" according to its neighbors during color refinement
		- nodes with the same color have same "$K$-hop neighborhood"

## Advantages
- super efficient: computing similarity is linear in # of edges
	- compared to graphlet kernels which are computationally expensive
- only colors that appeared in two graphs need to be tracked -- max colors just scales with total # of nodes
	- decide the range of the hash function ahead of time -- work with limited # of colors
- isomorphic graphs will have exact same coloring
- closely related to **Graph Neural Networks**

# Node Embeddings (new content)
- traditional ML pipeline: input graph -> structured features -> learning algo -> prediction
	- feature engineering is really significant
	- with graph ML: **automatically learn the feature representation**
- today: **task-independent feature learning for ML with graphs**
	- how do you take the graph and map into feature vectors / embeddings?

## Why learn rather than construct by hand?
- much more flexible
- less manual effort

## Task
- similarity of embeddings btwn nodes should reflect similarity w/in graph
- task-independent means we can potentially learn for many downstream predictions
- this is an *unsupervised / self-supervised* way to learn node embeddings
	- b/c not using node labels / features

## Setup
- $V$ is the vertex set
- $A$ is the (binary) adjacency matrix
- goal: encode nodes s.t. similarity in embedding space (dot product) approximates similarity in graph
	- how to define similarity in network?
	- how to encode? (encoder function)
	- **decoder** will map from embeddings to similarity
	- optimize encoder s.t. $similarity(u,v)\approx \mathbf{z}_v^T\mathbf{z}_u$

### Encoder
maps each node to $d$-dimensional embedding
$$ENC(v) = \mathbf{z}_v$$
- most naive approach: **embedding-lookup** (a "shallow" approach)
	- $ENC(v) = \mathbf{z}_v = \mathbf{Z}\cdot v$
	- $\mathbf{Z}\in\mathbb{R}^{d\times|V|}$: matrix; each column is node embedding
		- we learn / optimize this: "for every node, I have to learn these coordinates"
	- $v\in\mathbb{I}^{|V|}$: indicator vector, all zeroes except 1 in column indicating $v$
	- just assign each node a unique embedding vector that is learned directly through optimization
	- DeepWalk, node2vec, other methods
	- drawback: not inductive: doesn't generalize across different graphs

### Similarity function
specifies how relationships in vector space map to the relationships in original network
- definition uses random walks

## Random Walk Approaches to Node Embeddings
- Probability $P(v|\mathbf{z}_u)$
	- Predicted probability of visiting node $v$ on random walks starting from node $u$
	- Use non-linear functions to produce this predicted probability: `softmax` and `sigmoid`
		- **softmax**: exponentiate each coordinate, then normalize by sum of the exponentiated values
			- most of the probability goes to the coordinate that has the largest value (why it's a "soft version of maximum")
- all random walks will be same length (fixed # of steps)
- optimize: $\mathbf{z}_u^T\mathbf{z}_v\approx$ probability that $u$ and $v$ co-occur on a random walk
- very flexible stochastic definition for thinking about similarity between networks
	- incorporates both local and neighborhood information
	- *efficient*: don't consider all node pairs when training, just those that co-occur

1. estimate probability of visiting node $v$ from $u$, using a **random walk strategy** $R$ (biased estimate)
2. optimize embeddings to encode random walk statistics (co-occurrence of nodes)
	- learn embeddings so similar nodes are close together in network (form of **unsupervised feature learning**)

### Random Walk Optimization
- $N_R(u)$ is the *neighborhood* of node $u$ we obtain from random walks taken via strategy $R$
	- learn embedding mapping $f(u)=\mathbf{z}_u$
	- learn log-likelihood objective: $\max_f\sum_{u\in V}\log P(N_R(u)|\mathbf{z}_u)$
	- run short, fixed-length random walks, collect $N_R(u)$ as *multiset* of nodes visited

optimizing
$$\mathcal{L}=\sum_{u\in V}\sum_{v\in N_R{u}}-\log(P(v|\mathbf{z}_u))$$
parameterize $P(v|\mathbf{z}_u)$ using softmax:
$$P(v|\mathbf{z}_u) = (\exp(\mathbf{z}_u^T\mathbf{z}_v)) / (\sum_{n\in V}\exp(\mathbf{z}_u^T\mathbf{z}_n))$$
"for every node, take its neighborhood defined by random walk, then take minus log softmax between node and each neighbor... then optimize with respect to $\mathbf{z}$ (minimize $\mathcal{L}$)"
- problem: the normalization sum $\sum_{n\in V}\exp(\mathbf{z}_u^T\mathbf{z}_n)$ is too expensive! rewrite using **negative sampling**
	- send dot product to sigmoid, take log of that, then subtract sum over some $K$ nodes (negative samples $n_i\sim P_V$ drawn from some distribution)
		- replace entire calculation of softmax probability with this (see slide called "Negative Sampling" in [[Lecture 3 -  Node Embeddings.pdf|slides]])
	- samples $k$ negative nodes $n_i$ with probability proportional to degree (higher degree / more "popular" nodes selected more often)
		- "negative" technically means nodes not appearing on random walk, but in practice some people use all for efficiency of calculation
	- negative sampling allows for quick likelihood calculation -- normalizing has been replaced with much quicker summation of just some 10, 20, etc. nodes (instead of all, say, 1 million)
	- higher $k$ gives more robust estimates, but corresponds to higher bias on negative events
		- in practice, $5\leq k\leq 20$
- finally: optimize using stochastic gradient descent
	- for all $v$, update $\mathbf{z}_v\gets \mathbf{z}_v - \eta(\partial\mathcal{L}^{(u)}/\partial\mathbf{z}_v)$

### How to Randomly Walk?
#### DeepWalk algorithm
- so far: run fixed-length walks, picking each new node at random
	- this notion of similarity can be too constrained -- *how to generalize?*
	- DeepWalk algorithm
#### node2vec
- goal: embed nodes w/ similar network neighborhoods close in the feature space
- 2nd order Markov chain (biased)
	- trade off between local and global views of network (BFS vs. DFS)
		- BFS: micro-view of neighborhood
		- DFS: macro-view
		- goal: interpolate btwn strategies
	- use two strategies to define $N_R(u)$
- 2 parameters:
	- $p$ = return parameter (return back to previous node?)
	- $q$ in-out parameter = ratio between BFS and DFS
- idea: at given node, can go
	1. closer to starting node -> prob. $1/p$ (low value of $p$ means more BFS-like)
	2. at the same distance from start -> prob. 1
	3. move further -> prob. $1/q$ (low value of $q$ means more DFS-like)
	- have to remember where the walk came from (why it's called 2nd order -- 1 piece of memory, requires pre-computing distances)
- think of edge weights as non-normalized transition probabilities
- **linear-time complexity** b/c each of the 3 steps are individually parallelizable

no one method is best -- node2vec performs better on node classification, while other methods are better at link prediction

# Graph / Subgraph Embeddings
- Simple approach: just sum the node embeddings up to get the graph embedding!
- another approach: introduce "virtual node" to represent subgraph and run a standard graph embedding technique