#CS224W #CS #lecture-notes #Stanford #Stanford-Fall-2021 
See [[Lecture 8 - Applications of Graph Neural Networks.pdf|slides]]
___
# Agenda
- graph augmentation
	- [[202110182307 CS 224W Lecture 7 Notes (2021-10-12)|last time]] we defined an individual layer -- now we need to stack multiple layers
	- $h_0$ (level 0) embedding is just the features of the node
- learning objectives

# Stacking GNN Layers
- take inputs, create message, transform message, aggregate message, and you have a single layer embedding
- Simplest / standard way: stack GNN layers sequentially
	- after single GNN layer, every node has embedding/features at level 1 --> keep passing that through GNN layers
	- stop after $K$ steps -- that is your final node embedding

## Issues with Stacking
### Oversmoothing
- **oversmoothing**: all node embeddings will converge to same value
	- bad: we want to use node embeddings to differentiate nodes
	- if you collect information from too far apart... will all look the same when it gets to the central node
	- resolve with different ways, e.g. skip connections
- **receptive field**: the set of nodes that will determine embedding of node of interest
	- in $K$-layer GNN each node has receptive field = $K$-hop neighborhood
	- too large of a receptive field --> starts to become whole network
	- *shared neighbors* quickly grows when we increase number of hops (= GNN layers)
		- most information will be irrelevant for predictions we want to make...
	- determines embedding of a node
- stack many GNN layers -> nodes have highly-overlapping receptive fields -> node embeddings will be highly similar -> suffer from oversmoothing

#### How to Handle Oversmoothing Problem?
- unlike neural nets in other domains (e.g. CNNs), **adding more GNN layers does not always help**
- set num layers $L$ to be a bit more than receptive field we like
	- analyze necessary receptive field (e.g. take into account diameter of the graph)
- if problem still requires many layers -> add **skip connections**
	- increase impact of earlier layers in the final node embeddings by *adding shortcuts*
	- e.g. $h_v^{(l)} = F(x) + x$ (see slides)
	- allow you to create a *mixture of models*
		- $N$ skip connections -> $2^N$ possible paths
		- automatically get mixture of shallow GNNs and deep GNNs (behaves like an ensemble)
	- could also just skip straight to last layer

### How to make shallow GNNs more expressive?
- **increase expressive power within single GNN layer**
	- make aggregation / transformation become its own deep neural network
- **add layers that do not pass messages**
	- e.g. add MLP layers (applied to each node) as pre- and post-process layers

# Graph Augmentations for GNNs
- idea: raw input graph $\neq$ computational graph
	- supplement w/ feature & structure aggregation
	- input graph may lack features -> feature augmentation
	- graph may be too sparse / large / dense
		- -> add virtual nodes / edges
		- -> sample neighbors when message passing
		- -> sample subgraphs to compute embeddings

## Feature Augmentation
- needed when input graph does not have node features (e.g., only adjacency matrix)
- standard approaches
	- assign constant values to nodes
	- assign unique IDs to nodes (1-hot encodings)
		- no longer inductive (transductive setting only -- no new nodes)
		- high computational cost, $O(|V|)$ dimensional features w/ one-hot encodings
- cetain features hard to learn by GNNs
	- cycle count features: GNN cannot learn length of cycle that $v_1$ resides in
		- computational graph is the same, say if both graphs have only degree 2 (same binary tree)
		- solution: use cycle count as structural feature to augment node features
	- also:
		- node degree
		- clustering coefficient
		- PageRank
		- centrality
		- any feature from [[CS 224W/Lecture Notes/202110061645 CS 224W Lecture 2 Notes (2021-09-23)|lecture 2]] can be used

## Add Virtual Nodes / Edges
- motivation: **augment sparse graphs**
- adding virtual edges
	- connect 2-hop neighbors via virtual edges: use $A + A^2$
		- think about Katz index talked about in [[CS 224W/Lecture Notes/202110061645 CS 224W Lecture 2 Notes (2021-09-23)|lecture 2]]
	- use case: bipartite graphs
- adding  virtual nodes
	- creating virtual node connecting all nodes --> all nodes will have max distance of 2 (A -- Virtual Node -- B)
	- greatly improves message passing in sparse graphs

## Node Neighborhood Sampling
- remove edges from the graph to improve learning
- instead of using all nodes for message passing, randomly sampel node's neighborhood for message passing
- rather than dropout, think about how you're selecting the computation graph (both for training and for inference)
- in expectation, embeddings of similar nodes will be similar
	- but *greatly reduces computational cost* --> allows scaling to large graphs

# Prediction with GNNs
- learning objective
- prediction head -> predictions
	- node-level tasks
	- edge-level tasks
	- graph-level tasks
- **prediction heads** means "making a prediction" of some kind

## Node-Level Prediction
Have $d$-dim node embeddings after GNN computation
- suppose $k$-way prediction problem ($k$ categories or targets)
- $\mathbf{\hat{y}}_v = \text{Head}_{\text{node}}(\mathbf{h}_v^{(L)}) = \mathbf{W}^{(H)}\mathbf{h}_v^{(L)}$
	- $\mathbf{W}^{(H)}$ maps node mebeddings from $\mathbf{h}_v^{(L)}$ to $\mathbf{\hat{y}}_v$ so we can compute the loss

## Edge-Level Prediction
Learn prediction head that takes 2 nodes and outputs probability that they are connected
$$\mathbf{\hat{y}}_{uv}=\text{Head}_{\text{edge}}(\mathbf{h}_u^{(L)}, \mathbf{h}_v^{(L)})$$
- options for $\text{Head}_{\text{edge}}$
	1. concat + linear
		- e.g. from graph attention
		- $\mathbf{\hat{y}}_{uv} = \text{Linear}(\text{Concat}(\mathbf{h}_u^{(L)}, \mathbf{h}_v^{(L)}))$
	2. Dot product
		- this approach only applies to 1-way prediction
		- for $k$-way prediction: do something similar to multi-head attention, w/ trainable $\mathbf{W}^{(1)},\dots, \mathbf{W}^{(K)}$'s

## Graph-Level Prediction
$k$-way prediction using all node embeddings in our graph
$$\mathbf{\hat{y}} = \text{Head}_{\text{graph}}(\{\mathbf{h}_v^{(L)}\in\mathbb{R}^d,\forall v\in G\})$$
- similar to $\text{AGG}(\cdot)$ in a GNN!
- options for $\text{Head}_{\text{graph}}$
	- global mean pooling
	- global max pooling
	- global sum pooling

### Issue with Global Pooling
- global pooling over a large graph will lose information

### Solution: Heirarchical Pooling
- toy example: aggregate using $\text{ReLU}(\text{Sum}(\cdot))$
	- first separately aggregate first 2 ($x$) nodes and last 3 ($N-x$) nodes
	- then, aggregate again to make final prediction

#### DiffPool
- hierarchically pool node embeddings
	- keep doing this until you have only 1 node -- that's your embedding for the graph
- leverage 2 independent GNNs at each level / pooling layer
	- node embeddings and clusters that nodes belong to
	- creates new "supernode" (cluster) at each layer
	- can be executed in parallel

# Loss functions
## Supervised vs. Unsupervised
- **supervised learning**: labels come from external sources
	- specific use case, e.g. "subject area of a paper," "whether a transaction was fraudulent"
	- *reduce your task to node / edge / graph level task, since these are easy to work with*
- **unsupervised learning**: signals come from the graphs themselves
	- e.g. link prediction: predict if 2 nodes are connected
	- sometimes we don't have any external labels, just a graph
	- find supervision symbols
		- e.g. let GNN compute node statistics like clustering coefficient or PageRank
		- edge-level: hide edges between nodes and predict if there should be a link
		- graph-level: predict isomorphisms
- differences can be blurry!
	- still have "supervision" in unsupervised leraning (could be called "self-supervised")

## Final Loss for Backpropagation
- have $N$ data points; each is a node/edge/graph
	- use prediction $\mathbf{\hat{y}}^{(i)}$ and label $\mathbf{y}^{(i)}$ at all levels

### Classification Loss
- **cross-entropy loss**: very popular loss function for classification
	- take true label times log of predicted label
	- label is a 1-hot encoding
	- prediction is distribution of probabilities after $\text{Softmax}(\cdot)$

### Regression Loss
- convention: **mean squared error** (MSE) or **root mean squared error** (RMSE)
	- both real-valued vector of targets & predictions