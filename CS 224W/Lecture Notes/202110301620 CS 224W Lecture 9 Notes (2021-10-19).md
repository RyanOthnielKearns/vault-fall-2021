#CS224W #CS #lecture-notes 
See [[Lecture 9 - Theory of Graph Neural Networks.pdf|slides]]
# "Theory of Graph Neural Networks"
___
*What are GNNs / what are the components of the architecture?*
# Finishing GNN Training Pipeline
## Intricacy: how to "split" the data?
- graphs are unlike other datasets in that they don't admit to "splitting as easily"
- different from splitting image dataset
	- in graphs: data points not independent: do I keep edges going from training to test set? --> a lot of connectivity "thrown out"

### Fixed split
- **training set**: used for optimizing GNN parameters
- **validation set**: develop model / hyperparams
- **test set**: held out until report final performance
- concern: cannot guarantee that test set will really be held out (in graphs)

### Random split
- randomly split into three subsets
- report average performance over multiple splits

### Solution 1: Transductive Setting
- input graph can be observed in all dataset split
	- only split, e.g. (node) labels
- on training, train only using a subset of node labels
- at validation time, still compute embeddings on entire graph but evaluate on node labels held out of training
- all operating on single graph w/ dependencies btwn sets
- only applicable to node / edge prediction tasks

### Solution 2: Inductive Setting
- make different sets truly independent of each other; get (3) truly separate graphs
	- break edges between splits
- successful model should generalize to unseen graphs (really "testing the generalization")
- applicable to node / edge / graph-level prediction tasks
- problem: "chopping" a single graph is non-trivial
	- for graph classification, very easy! just test on unseen graphs

## Link Prediction
- setup is tricky... unsupervised / self-supervised task
- hide some edges from GNN and let GNN predict if those edges exist
- split edges twice
	1. assign 2 types of edges in original graph
		1. **message edges** used for GNN message passing
		2. **supervision edges** for computing objectives
	2. split edges into train / validation / test
		- only message edges remain in the graph --> supervision edges used as supervision for edge predictions (not used by GNN!)
		- two options
			1. inductive link prediction -- each inductive split contains independent graphs
				- each graph has message or supervision edges
			2. transductive setting
				- "default setting" for link prediction -- have single graph dataset
				- if graph evolves over time, chop time into different buckets and use time slices to split train / val / test
				- by definition of "transductive," entire graph can be observed at all dataset splits
					- use training edges to predict other training edges
					- use training edges + supervision edges to predict validation edges
					- use all other edges to predict test edges

# When Things Don't Go As Planned
- tips for debugging
- **data preprocessing is important**
	- node attributes may vary a lot -- potentially high range
		- can flood gradients & make it hard to learn
	- use normalization (logs, etc.)
- **optimizer**
	- empricially, ADAM
- **activation function**
	- empiricially, ReLU activation
- include **bias term** at every layer

## Debugging Tips
- loss / accuracy not converging?
	- check pipeline, check values of gradients
	- adjust hyperparams to correct order of magnitude
- for model development
	- first, *overfit* on part of training data
		- should be able to smush loss to 0 with sufficiently expressive neural network
- scrutinize loss functions / visualizations

## Resources
- pytorch
	- PyG (GraphGym) and DGL
- tensorflow
	- GraphNets
- great resources on [[Lecture 9 - Theory of Graph Neural Networks.pdf|slides]]!

# How Expressive are Graph Neural Networks?
- what can they learn / not learn?
	- how to design the maximally expressive one?
- which GNN is the most expressive?

## Formulation
- use node colors to represent node feature values
	- example: graph with only 1 color --> all nodes have *same feature*
	- if you have really good features, don't really have to worry about graph structure...
- consider **local neighborhood structures**
	- e.g., degrees to distinguish nodes
		- certain nodes can't be distinguished by neighborhood structure if *symmetrical* within graph
- key question: *when can a GNN distinguish different local neighborhood structures?*
	- key will be difference in computational graphs
		- symmetrical nodes have identical computational graph structures
			- if node features are identical, GNN cannot distinguish

## Computational Graphs
- GNN's node embeddings capture **rooted subtree structures**
	- map different rooted subtrees into different node embeddings
		- *injective* function $f: X\rightarrow Y$
		- $f$ *retains all the information about the input*
		- if GNN's aggregation can fully retain neighboring information, generated node embeddings can distinguish different rooted subtrees --> use **injective neighbor aggregation** function at each step
	- subtrees of same depth can be *recursively characterized* from leaf notes to root nodes

# Designing the Most Powerful GNN
- expressive power can be characterized by that of neighborhood aggregation functions they use
	- injective aggregation function leads to most expressive GNN

- **neighborhood aggregation** abstracted as function over a multi-set (set w/ repeating elements)

## Analyzing two Neighborhood Aggregations
### GCN (mean-pool)
- Theorem: agg function cannot distinguish btwn multi-sets with same proportions of node "colors" (features)

### GraphSAGE (max-pool)
- Theorem: agg function cannot distinguish btwn different multi-sets with the *same set of colors*
	- not injective, therefore not the most expressive possible

## The most expressive message passing GNN
- design injective neighbor aggregation function
**Theorem**: any injective multi-set function can be expressed as
$$\Phi\bigg(\sum_{x\in S}f(x)\bigg)$$
where $\Phi$ and $f$ are non-linear functions
- intuition: $f$ produces a one-hot encoding for the "colors" -- summation of one-hot encodings retains all information about the input multi-set

**Universal Approximation Theorem**: 1-hidden-layer MLP w/ sufficiently large hidden dimensionality and approximate non-linearity can *approximate any continuous function to an arbitrary accuracy*.
- model $f$ and $\Phi$ using a MLP via this theorem
$$\text{MLP}_\Phi\bigg(\sum_{x\in S}\text{MLP}_f(x)\bigg)$$
- called **Graph Isomorphism Network (GIN)**
	- apply MLP, element-wise sum, followed by another MLP
	- the most expressive GNN of all message-passing GNNs!

### GIN <> WL Graph Kernel
- GIN is a "neural network" version of the WL-kernel
- **WL-Kernel** (described in [[202110072334 CS 224W Lecture 3 Notes (2021-09-28)#WL Kernel continuing from 202110061645 CS 224W Lecture 2 Notes 2021-09-23 previous lecture|lecture 3]]), is a color-refinement algorithm
	- $c^{(k+1)}(v) = \text{HASH}(c^{(k)}(v), \{c^{(k)}(u)\}_{u\in N(v)})$
		- after $K$ steps of color refinement, $c^{(k)}(v)$ summarizes the structure of the node's $K$-hop neighborhood
		- assume HASH table is injective!
	- two graphs are isomorphic if they have the same set of colors
	- comparison with GIN
		- $c^{(k)}(v)$: root node feature
		- $\{c^{(k)}(u)\}_{u\in N(v)}$: neighboring node features
		- if input feature is represented as one-hot encoding, *direct summation is injective*
		- update function is $\text{GINConv}$ instead of $\text{HASH}$
		- update target are node embeddings instead of (one-hot) node colors

#### Expressive Power
- exactly the same! distinguish exactly the same class of graphs
- empiricially: distinguishes most real-world graphs

# Power of Pooling
in terms of expressive power:
max - set < mean - distribution < sum - multiset

there are still some things like cycle length that GNNs cannot capture