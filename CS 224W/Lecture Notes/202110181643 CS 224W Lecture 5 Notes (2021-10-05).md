#CS #CS224W #lecture-notes 
# Agenda
- random walks, node embeddings, PageRank
- today: graph neural networks
	- but first: message passing and node classification
- **Main question**: given network with labels on some nodes, how do we assign labels to other nodes on network?
	- *semi-supervised node classification*
- **Message Passing**: correlations (dependencies) exist in the network
	- "similar nodes are connected"
	- *collective classification*: assign labels to all nodes "together" (in dependent way)
	- 3 techniques:
		- **Relational Classification**
		- **Iterative Classification**
		- **Belief Propagation**

# Correlation
Nearby nodes belong to the same class
- 2 explanations:
	1. **Homophily**
		- Individuals that are similar tend to associate and bond
		- "birds of a feather flock together"
	2. **Influence**
		- social connections influence individual characteristics
			- kind of the opposite direction to homophily

## How to leverage correlations in networks?
- "guilt-by-association": if I'm connected to node w/ label $X$, I'm likely to have label $X$
- classification label for $v$ may depend on
	- features of $v$
	- labels / features in $v$'s neighborhood
- main assumption: homophily (equivalently "smoothness")

# Semi-supervised learning formulation
- Graph $G = (V, E)$
- $A$ is $n\times n$ adjacency matrix
- $Y = \{0, 1\}^n$ is vector of labels
- Each node $v$ has feature vector $f_v$
- *Task*: find $P(Y_v)$

## Three Approaches
### Relational Classification
- propagage node labels across network
- class probability is weighted average of class probs. of neighbors
- for unlabeled nodes, initialize $Y_v=0.5$.
- update all nodes in random order until convergence

#### Challenges
- a bit ad hoc: convergence not guaranteed
- model cannot use node feature info

### Iterative Classification
- improve over relational classification by also considering node attributes (features)
- key idea: compute 2 feature vectors
	- $\phi_1$ to predict node labels based on just their own feature vectors
		- used to "bootstrap" classification task (once iterating, rely only on $\phi_2$)
	- $\phi_2$ to predict node labels based on feature vectors and summaries ($z_v$) of labels of $v$'s neighbors
- iterate until class labels stabilize or some max num. iters (convergence not guaranteed)
	1. update relational features $z_v$
	2. update labels $Y_v$
- summary may start out incomplete if you don't have labels for all nodes -- need to know how to handle incomplete inputs / features

### Collective Classification (Correct & Smooth)
1. train base predictor (like $\phi_1$)
2. predict soft labels (probability of class belonging) of nodes
	- even labeled nodes!
3. post-process the predictions using graph structure to get final predictions

#### Post-Processing Steps
these steps post-process soft node labels predicted by any base model
1. correct step
	- intuition: base model makes *different degrees of error* at different locations in graph
		- errors are *biased* -- correct for it!
		- assumption: prediction errors are similar for nearby nodes
	- compute **training errors**: ground-truth minus soft label
		- 0 for unlabeled nodes
	- diffuse training errors $\mathbf{E}^{(0)}$ along edges
	- $\tilde{A}$ is the diffusion matrix from adjacency matrix $A$
		- $\tilde{A}\equiv D^{-1/2}AD^{-1/2}$
	- update $\mathbf{E}^{(t+1)}$ similar to PageRank (see slides)
		- $\tilde{A}\mathbf{E}^{(t)}$ "diffuses errors to neighboring nodes" --> propagate to neighboring unlabled nodes
	- add errors to soft label predictions (mediated by some scale hyperparameter $s$)
2. smooth step
	- predicted soft labels may not be smooth -- average them, reduce variance
	- *assumption*: neighboring nodes tend to share same labels
	- for training nodes, now use ground-truth hard labels instead of soft labels (reinitialize)
	- run similar smoothing iteration to correct step
		- diffuse label $\mathbf{Z}^{(0)}$, propagate in similar way to PageRank
	- empirically more important for improving accuracy