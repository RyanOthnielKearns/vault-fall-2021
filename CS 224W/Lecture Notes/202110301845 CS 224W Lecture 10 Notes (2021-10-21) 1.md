#CS224W #CS #lecture-notes 
See [[Lectrure 10 - Knowledge Graph Embeddings.pdf|slides]]
# "Knowledge Graph Embeddings"
___
# Overview
discussion of heterogeneous graphs & knowledge graphs
- graphs will multiple edge types
	- relational GCNs
	- knowledge graphs
	- embeddings for KG completion

# Heterogeneous Graphs
$$G = (V, E, R, T)$$
- edges with relation types $(v_i, r, v_j)\in E$
- node type $T(v_i)$
- relation type $r\in R$

## Extending GCN to Heterogeneous Setting
**Relational GCN (RGCN)**
- ideas can also be extended easily to other message-passing architectures (RGraphSAGE, RGAT, etc.)
- Recall single GNN layer
	1. Message function to produce $\mathbf{m}_u^{(l)}$
	2. Aggregation of messages from neighbors to get embeddings $\mathbf{h}_v^{(l)}$
	- non-linearity for expressiveness
	- GCN: message created with averaged embeddings of neighboring nodes, multiplied via transformation matrix $\mathbf{W}$
		- sum aggregation and $\sigma(\cdot)$ non-linearity

### Multiple Relation Types
- have multiple weight matrices $\mathbf{W}_r$ for different relation types

Relational GCN:
$$\mathbf{h}_v^{(l+1)}=\sigma\bigg(\sum_{r\in R}\sum_{u\in N_v^r}\frac{1}{c_{v,r}}\mathbf{W}_r^{(l)}\mathbf{h}_u^{(l)}+\mathbf{W}_0^{(l)}\mathbf{h}_v^{(l)}\bigg)$$
- explicity write out "self-loop" with $\mathbf{W}_0\mathbf{h}_v^{(l)}$ formulation
- aggregation: sum over messages from neighbors and self-loop

### Scalability
Rapid # of params w.r.t. # relations! Overfitting becomes an issue
- 2 methods to regularize weights $W_r^{(l)}$
	1. block diagonal matrices
	2. basis / dictionary learning

#### Block Diagonal Matrices
- make the weights sparse!
	- only nearby neurons can communicate with each other through $\mathbf{W}$

#### Basis Learning
- share weights across different relations!
- represent matrix of each relation as linear comb. of basis transformations
	- $\mathbf{W}_r=\sum_{b=1}^B a_{rb}\cdot\mathbf{V}_b$ where $\mathbf{V}_b$ shared across all relations
- each relation only nees to learn importance weihts $a_{rb}$, which is $B$ scalars

### Prediction
#### Node Classification
- $\mathbf{h}_A^{(L)}$ is the probability each class

#### Link Prediction
- output different probability / prediction for each relation type --> separate relation head w/ relation-specific parameters
##### Training
- use RGCN to score training supervision edge
- create a *negative edge* by perturbing the supervision edge (that cannot belong to training message / supervision edges)
- use GNN model to score the positive and negative edges
	- optimize a cross entropy loss -- minimize score of negative edge as well

##### Evaluation
- validation edge score should be higher than all other non-tails of this relation
- have to learn validation performance by ranking rather than "accuracy," due to massive class imbalance between positive and negative cases for each relation

# Knowledge Graphs
- knowledge in graph form: entities, types, and relations
	- nodes are *entities*, labeled with *types*; edges capture *relations*
- lots of large knowledge graphs used in industrial practice
	- for serving information / answering queries
	- commonly *massive* and *incomplete* (many true edges missing)
		- e.g. 93.8% people in FreeBase have no place of birth listed!
		- in practice, researchers use complete subsets

## Knowledge Graph Completion
- **task**: for given (head, relation), predict missing tail
- recall shallow encoding ([[202110072334 CS 224W Lecture 3 Notes (2021-09-28)#Encoder]])
	- not using GNN to construct embedding - just learn embedding directly
- edges in KG are triples $(h, r, t)$
- **Key Idea**: model entities / relations in embedding / vector space $\mathbb{R}^d$

## TransE
- for triple $(h, r, t)$:
	- $\mathbf{h}+\mathbf{r}\approx\mathbf{t}$ if given fact is true
	- $\mathbf{h}+\mathbf{r}\neq\mathbf{t}$ otherwise
- scoring function: $f_r(h, t)= -\vert\vert\mathbf{h}+\mathbf{r} - \mathbf{t}\vert\vert$
- training alg
	- sample positive and negative (corrupted) triplets
	- update embeddings w.r.t. *contrastive loss*: favor low distance for valid triplets / high distance for invalid ones

### Connectivity Patterns
- **symmetric** relations e.g. "roommate_of"
	- TransE *cannot* model -- $\mathbf{h = t}$ only if $\mathbf{r} = 0$ -- but $h$ and $t$ are different entities, so need to be in different places in the space
- **inverse** relations e.g. "advisor / advisee"
	- TransE can model
- **antisymmetric** relations e.g. "hypernym"
	- TransE can model
- **transitive** relations -- TransE can model (just rely on vector additivity)
- **1-to-N** relations cannot be modeled (e.g. "students_of")

## TransR
- can we instead to transformation in relation-specific space, for a given relation?
- models entities as vectors in the entity space -> each relation is a vector in the relation space
	- relation space $\mathbf{r}\in\mathbb{R}^k$
	- $\mathbf{M}_r\in\mathbb{R}^{k\times d}$ as the **projection matrix**

$$\mathbf{h}_\bot = \mathbf{M}_r\mathbf{h}, \mathbf{t}_\bot = \mathbf{M}_r\mathbf{t}$$
- use $\mathbf{M}_r$ to project from entity to relation-space, then effectively do TransE

### Connectivity Patterns
- *can model symmetric relations*: let $\mathbf{M}_r$ map two different entity points to the same place in the relation space $\mathbf{r}$
- *can model 1-to-N relations*: learn a matrix that maps entities to the same place in the relation space, then just translate w/ a learned vector
- can also model inverse relationships: translation vectors will just be opposites
- *cannot* model composition relations
	- once you project into a certain relational space, you cannot know how to "project back" in order to preserve composition to a second relation
	- likewise holds for transitive relations

## DistMult
- idea: **bilinear modeling**
- entities and relations use vectors
- score function is different:
$$f_r(h, t)=\langle\mathbf{h},\mathbf{r},\mathbf{t}\rangle=\sum_i\mathbf{h}_i\cdot\mathbf{r}_i\cdot\mathbf{t}_i$$
- can be viewed as *cosine similarity* between $\mathbf{h}\cdot\mathbf{r}$ and $\mathbf{t}$

### Connectivity Patterns
- can model 1-to-N relations: just put all tails valid for the head on the same side of the hyperplane, s.t. cosine similarity is positive
- can naturally model symmetric relations (order of vectors in product does not matter)
	- for this reason *cannot model antisymmetric relations*
	- related: *cannot model inverse relations*: would require vectors for the relations to be the same, but these are different relations
- *composition relations*: cannot do
	- union of hyperplanes induced by multi-hop of relations cannot be expressed using a single hyperplane

## ComplEx
- like Distmult, embed entities and relat ions using complex vector space (vectors in $\mathbb{C}^k$)
- score function:
$$f_r(h, t) = \text{Re}(\sum_i\mathbf{h}_i\cdot\mathbf{r}_i\cdot\mathbf{\bar{t}}_i)$$
- take real component of this product

### Connectivity Patterns
- can model antisymmetric relations
	- expressive enough b/c of asymmetric modeling w/ complex conjugate
- can also model symmetric relations (when $\text{Im}(\mathbf{r})=0$)
	- set imaginary part to 0
- can model inverse relations: $r_2(h, t)\Rightarrow r_1(t, h)$
	- $\mathbf{r}_1=\bar{\mathbf{r}}_2$
- like DistMult, cannot model *composition* relations, but can model *1-to-N* relations

## In Practice
- different KGs have drastically different relation patterns
	- no general embedding that works best for all KGs
	- good rule of thumb: try TransE because quick / easy to implement (if you don't have many symmetric relations)
- basically none of the relation models can do *hierarchical* relations
	- to do this, need to move to a hyperbolic geometry