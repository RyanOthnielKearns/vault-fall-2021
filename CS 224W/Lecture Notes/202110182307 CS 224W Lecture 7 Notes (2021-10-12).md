#CS #CS224W #lecture-notes #GNNs #graphs #Stanford #Stanford-Fall-2021 
# Agenda
## Project
- For context on state-of-the-art: check [OGB Leaderboard](https://ogb.stanfordd.edu/docs/leader_overview/)
- Top ML conference papers:
	- KDD
	- ICLR
	- ICML
	- NeurIPS
	- WWW
	- WSDM
- Project Proposal includes:
	1. application domain
	2. graph ML technique to apply

# Recap: Graph Neural Networks
- aggregations are learned via neural networks
- computation graph will be different for every node
	- plan for message transformations / message routing will be different
	- unique for neural networks! tend to think of computational architecture as fixed

## Why do GNNs generalize other NNs?
- from last time: permutation invariance and equivariance

### Convolutional Neural Networks
- CNN layer with 3x3 filter: $N(v)$ represents 8 neighbor pixels of $v$
	- CNN formulation looks like a GNN formulation
	- special case of a GNN with fixed neighbor size and ordering
		- size of filter pre-defined
		- not permutation invariant (while general GNN is)

### Transformer Architecture
- great performance on sequence modeling tasks
- key component: **self-attention** (each token / word attends to other tokens via matrix calculation)
- see transformer layer as a special GNN running on fully connected word graph
	- because each word attends to each other, *computation graph* is identical

# A General Perspective on Graph Neural Networks
- GNN **Layer** = Message + Aggregation
	- layers stacked sequentially (can add skip connections)
		- this is the third component, "Layer Connectivity"
	- raw input graph $\neq$ computational graph (feature + structure augmentations)
	- how to define the learning objective?
## Building Blocks
1. Message
2. Aggregation
3. Layer connectivity
4. Graph augmentation
5. Learning objective

## A Single GNN Layer
= Message + Aggregation
- idea: compress vectors into single vector in permutation invariant way
### Message Computation
- Message function $\mathbf{m}_u^{(l)} = \text{MSG}^{(l)}\big(\mathbf{h}_u^{(l-1)}\big)$

### Aggregation
$$\mathbf{h}_v^{(l)} = \text{AGG}^{(l)}\big(\big\{\mathbf{m}_u^{(l)}, u\in N(v)\big\}\big)$$
- ex. sum, mean, or max aggregator
- issue: information from node itself ($v$) could be lost
	- computation does not directly depend on embedding of previous level
	- include it! usually, make different message computation, then aggregate the separate computation using concat or sum

### Nonlinearity activation
Adds expressiveness
- e.g. ReLU, sigmoid, etc. --> written as $\sigma(\cdot)$

## Classical GNN Layers
### GCN: Graph Convolutional Networks
$$\mathbf{h}_v^{(l)} = \sigma\bigg(\mathbf{W}^{(l)}\sum_{u\in N(v)}\frac{\mathbf{h}_u^{(l-1)}}{|N(v)|}\bigg)$$
- $\mathbf{W}$ is a learned matrix transformation
- message: each neighbor gets: $\mathbf{m}_u^{(l)}=\frac{1}{|N(v)|}\mathbf{W}^{(l)}\mathbf{h}_u^{(l-1)}$
	- normalized by node degree
- aggregation: sum
	- actual paper added self-loops to the graph to persist info about nodes in their own embeddings

### GraphSAGE
- message is computed within the $\text{AGG}(\cdot)$
	- specific aggregator not specified: can do *mean*, *pool* (e.g. MLP), *LSTM* with randomly shuffled list of neighbors...
- two stage aggregation:
	1. aggregate from node neighbors
	2. further aggregate over the node itself (concat with self-message, then do nonlinear activation)
- L2 normalization of embedding at every layer
	- without normalization, embedding vectors have different scales

### Graph Attention Networks (GAT)
- in aggregation, have attention weight for every edge
	- $\alpha_{vu}$ is weighting factor
		- in GCN / GraphSAGE, this is just inverse node degree
			- means all neighbors are equally important, which may not be true...
		- defined explicitly based on graph structural properties (like node degree) (meaning: not "learned")
			- can transfer function that computes $\alpha$ across to different graphs
- goal: specify arbitrary importance to different neighbors of each node in the graph
	- computed as byproduct of attention mechanism $a$
		- $a$ computes *attention coefficients* $e_{vu}$ across pairs of nodes based on their messages (at a particular layer)
		- normalize $e_{vu}$ to get final attention weight $\alpha_{vu}$ (using softmax "normalization")
$$\mathbf{h}_v^{(l)} = \sigma\big(\sum_{u\in N(v)}\alpha_{vu}\mathbf{W}^{(l)}\mathbf{h}_u^{(l-1)}\big)$$
What is the function $a$?
- $e_{AB}=a(\mathbf{W}^{(l)}\mathbf{h}_A^{(l-1)}, \mathbf{W}^{(l)}\mathbf{h}_B^{(l-1)})$
	- $=\text{Linear}(\text{Concat}(\mathbf{W}^{(l)}\mathbf{h}_A^{(l-1)}, \mathbf{W}^{(l)}\mathbf{h}_B^{(l-1)}))$
- paper is agnostic to choice of $a$ -- could be a single-layer neural network (e.g. with weights in the Linear layer)
	- parameters of $a$ are learned jointly with weight matrices in end-to-end fashion

#### Multi-Head Attention
- idea: have multiple attention scores, each a replica with different set of params
	- outputs are aggregated
- stabilizes the learning process of attention mechanism

#### Benefits
- computationally efficient
- storage efficient
- localized
- inductive capacity

## Other techniques
- **batch normalization**: stabilize neural network training
- **dropout**: prevent overfitting
- **attention / gating**: control the importance of a message

### Batch normalization
- re-center node embeddings to 0 mean and unit variance

### Dropout
- goal: regularize and prevent overfitting
- during training, with some probability, randomly set neurons to 0
- during testing, all neurons used for computation
- in GNNs:
	- dropout applied to linear layer in the message function (e.g. $\mathbf{m}_u^{(l)} = \mathbf{W}^{(l)}\mathbf{h}_u^{(l-1)}$)

### Activation
- apply activation at $i$-th dimension of embedding $\mathbf{x}$
- parametric ReLU: $\text{PReLU}(\mathbf{x}_i)=\max(\mathbf{x}_i, 0)+a_i\min(\mathbf{x}_i, 0)$
	- $a_i$ is trainable

==nice paper: "Design Space of Graph Neural Networks"==
==also check out `GraphGym` feature from `PyG`==