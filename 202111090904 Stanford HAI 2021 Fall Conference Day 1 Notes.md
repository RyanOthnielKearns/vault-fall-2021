# Policy & AI: Four Radical Proposals for a Better Society
## Introduction
### [Erik Brynjolfsson](https://hai.stanford.edu/people/erik-brynjolfsson)
Director, DIgital Economy Lab
- key question is what *we* will do with technology (not what technology is going to do to us)

### [Daniel E. Ho](https://hai.stanford.edu/people/daniel-e-ho-0)
Professor of Law and Political Science; Associate Director, HAI
- Chose 4 radical proposals for the future w/ AI
- not thinking that presenting these proposals will change society --> proposing them is a step towards awareness / catalyzing creative work on these problems

## Keynote Speaker: Eric Lander
[Eric Lander](https://www.whitehouse.gov/ostp/directors-office/)
- Presidentâ€™s Science Advisor and Director of the White House Office of Science and Technology Policy (OSTP)
- for AI: being radical means going to the *root* -- what are the principles, what are the rights?
- by training: mathematician and biomedical scientist
	- in theory: why does AI work well?
		- e.g. papers on "double descent," overparameterization overcoming the classic overfitting problem
- issues w/ powerful AI technology
	- facial recognition / medical outcome prediction algorithms w/ racial bias
	- hiring tools (learn features of *existing* employees and propagate)
- *what would be a bill of rights for people living in an AI-powered world?*
	- after ratifying constitution, America created a bill of rights to protect against the powerful government they had just created
		- so for the powerful technologies we're now creating today
	- being able to regain control of an AI system that's run amuck
	- what do we mean by bill of rights?
		- focused on systems that make *direct judgements* about benefits or harms to individuals
			- like how the FDA regulates medical products -- when they're going to directly influence *people*
			- interested in rights (with *teeth*), not just best practices
				- laws, litigation --> things that are practical / can protect people
				- governance, privacy, protection of personal data, transparency, accountability, etc.
		- not focused on AIs that, say, control energy efficiency, do protein folding, play Go, etc.
			- or: search application -- code deployed in products
- a *right to govern your own personal data*
	- currently: notice & consent -- not it!
		- have to give over all data, or can't use service or apps
		- relatively limited choices
		- limited time to read through everything "noticed"
	- how to turn into a real right?
		- passing on restrictions to 3rd parties that have and use data
		- right to withdraw data retrospectively
		- where people can delegate the operation of their right to an organization on their behalf
		- how do we make consent meaningful? how do we ensure user's permissions are actually upheld?
- a *right to fairness*?
	- challenges in defining algorithmic fairness: about 21 different definitions (from some 2018 FAT paper)
	- should datasets to create the AI be fully described?
		- should they be properly representative? --> will require oversampling on particular populations, rather than just using convenient ones
	- are the inputs appropriate to the task?
	- are the outcomes appropriate to the task?
		- some AI are trained to predict whether indivduals will be *arrested* for a particular crime (as opposed to *actually* committing a crime)
		- not appropriate if credit firm has an overall high accuracy -- look at comparable accuracy between groups
- 3rd party auditing?
	- should you require that AI itself be available for testing by *anyone*?
		- like bugfinding / bug bounty
		- civil society organizations may take that responsibility for the public
- =="*transparency* is a means to an end, and the end is trust"==
	- CS community has spent years building systems that we can trust and verify
	- have to do this for large & complex systems today
- right to *explainability*
	- when we consider AIs making decisions about humans
		- people have a right to an explanation of the behavior that is legitimate
		- needs to be clear and understandable -- a must-have for legitimacy
		- interpretable models will be seen as more legitimate
	- still need explanations even when more complex systems vastly increase performance on the task
		- attribute decisions to human-understandable factors --> human accountability

- requirement: incorporating rights *by design*, from the start
	- ensure developers of AI have diverse range of perspectives & experiences
		- does a failure at this entail a failure to exercise due care in the design of AI?
- Fei-Fei Li: advocating instead for bill of rights concerning scope of all technologies that are information & compute-focused, rather than just AI
	- Eric Lander response: true, but we have to start somewhere! narrow scope is correct place to start
		- also doesn't think that "bill of rights" is the right reaction to all technological developments
		- AI also touches a lot of realms where law has conventionally considered rights *in*, so it's a natural fit
- Daniel Ho: code availability <> open science (example: human genome project)
	- in AI: shift where most of the research is being done in industry
		- 2/3 of PhD's enter industry, compared w/ academy
	- Eric: human genome should be a public good -- available for so many varied applications / innovations / improvements in quality of life
		- at the same time, a specific drug relying on human genome insights deserves protection from freeloading financiers
		- it's about getting the balance right
		- some AI scientists see the benefit of a "vibrant public ecosystem" that they themselves can benefit from
			- that includes people in the corporate world

==stopped right before discussion on middleware==