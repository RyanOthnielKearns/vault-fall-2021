#Stanford #Stanford-Winter-2022 #CS #CS224N #lecture-notes #NLP 
___
# Lecture Plan
1. What is question answering?
2. Reading comprehension
3. Open-domain (textual) question answering

# Question Answering
- build systems that *automatically* answer questions posed *by humans in natural language*
	- one of the earliest domains in natural language understanding!
		- Simmons et al. 1964: first propsed using dependency parsing
- builds on **information sources** like text passages, web documents, knowledge bases, tables, etc.
- **question types**: factoid vs. non-factiod, open vs. closed domain, simple vs. compositional
- **answer types**: a short text segment, a paragraph, a list, binary yes/no, etc.
- really important for practical applications: Google search e.g., especially on mobile w/ limited UI
	- also speech systems, virtual assistants

## IBM's Watson
1. Question processing
2. Candidate answer generation
3. Candidate answer scoring
4. Confidence merging and ranking

## Now: QA in Deep Learning Era
- train large NN end-to-end with corpus of questions and correct answers

today: largely QA over **unstructured text**
- IBM relied on a lot of tabular form data

# Reading Comprehension
- **reading comprehension**: comprehend a passage of text and answer questions about its content
	- $(P, Q)\to A$
		- passage, query to answer
## Practical Applications
- good for evaluating NLU in general
	- Wendy Lenhert, 1977: "Since questions can be devised to query any aspect of text comprehension, the ability to answer questions is the strongest possible demonstration of understanding"
		- reduces many tasks to reading comprehension:
			- relation/info. extraction
			- semantic role labeling (recipients, instruments, goals, etc. -- the different roles of a verb)

## SQuAD
**Stanford Question Answering Dataset**
- 100k annotated (passage, question, answer) triples from crawled Wikipedia data
	- formed using mechanical turking
	- passages usually 100-150 words
	- each answer is always a span taken from the passage
- "almost solved" today: SoTA gets really high accuracy
- **evaluation**: exact match (0-1) or F1 measure of overlap
- other QA datasets: TriviaQA, Natural Questions (drawn from frequently asked Google search questions), HotpotQA (getting information from two pages using a multistep query)
	- SQuAD questions weren't "natural" -- generated by turkers

## Neural models for reading comprehension
- input: $C = (c_1,\dots, c_N); Q = (q_1,\dots, q_M); c_i, q_i\in V$
- Output: $1 \leq \text{start}\leq \text{end}\leq N$
	- bounds of the answer within the context $C$
- 2016-2018: family of LSTM-models w/ attention
- 2019+: BERT-like fine-tuned models

## LSTM-Based Models
Recap: **seq2seq** Model with **Attention**
- passage and question of very imbalanced lengths
	- goal: model which words in the passage are most relevant to the question
- don't need an autoregressive decoder this time! just train two classifiers to learn to predict the start and end tokens of the answer in the passage

### Encoding
- use concat of word embeddings and character embeddings (CNNs over characters) for each word in context and query
- then, use two bidirectional LSTMs separately to produce contextual embeddings for context & query

### Attention flow layer
- context-to-query attention: for each context word, choose most relevant words among the query words using attention
	- sort of the opposite to what you might think -- why look into query instead of context?
- query-to-context attention: choose context words most relevant to *one of* the query words
	- final output:
$$\mathbf{g}_i = [c_i; q_j; a_i\circ a_i; c_i\circ b]\in\mathbb{R}^{8H}$$
	- using Hadamard product is really effective in feed-forward, because further layers can reconstruct that relationship if given the product in some way...
1. Compute a similarity score for every pair $(c_i, q_j)$:
$$S_{i,j}=\mathbf{w}_\text{sim}^\top[\mathbf{c}_i;\mathbf{q}_j; \mathbf{c}_i\circ\mathbf{q}_j]\in\mathbb{R}$$
	- $\mathbf{w}_\text{sim}\in\mathbb{R}^{6H}$
2. Context-to-query attention
$$\alpha_{i,j}=\text{softmax}_j(S_{i,j})\in\mathbb{R}$$
$$a_i = \sum_{j=1}^M \alpha_{i,j}q_j\in\mathbb{R}^{2H}$$
3. Query-to-context attention
$$\beta_i=\text{softmax}_i(\max_{j=1}^M(S_{i,j}))\in\mathbb{R}^N$$
$$\mathbf{b} = \sum_{i=1}^N\beta_i c_i\in\mathbb{R}^{2H}$$
### Modeling and output layers
- more context-sensitive NN stuff! to process what we've learned in a sequence model towards producing our answer
- **modeling layer**: 2 more bi-LSTMs that run across the $\mathbf{g}_i$
	- input is $8H$, reduce output dim down to $2H$
	- modeling interactions within context words
- **output layer**: two classifiers to predict the start and end positions
	- $p_\text{start}=\text{softmax}(\mathbf{w}_\text{start}^\top[\mathbf{g}_i; \mathbf{m}_i])$
	- $p_\text{end}=\text{softmax}(\mathbf{w}_\text{end}^\top[\mathbf{g}_i; \mathbf{m}_i'])$
		- where $\mathbf{m}_i' = \text{BiLSTM}(\mathbf{m}_i)\in\mathbb{R}^{2H}$
- final training loss is log loss:
$$\mathcal{L}=-\log p_\text{start}(s^*) - \log p_\text{end}(e^*)$$

## BERT for reading comprehension
- **question**: segment A
- **passage**: segment B
- **answer**: predicting two endpoints in segment B

- $p_\text{start}=\text{softmax}(\mathbf{w}_\text{start}^\top\mathbf{h}_i)$
- $p_\text{end}=\text{softmax}(\mathbf{w}_\text{end}^\top\mathbf{h}_i)$
- where $\mathbf{h_i}$ is the final hidden state of the BERT encoder output for $c_i$

### Asymmetry
- BERT base transformer has huge # of params (100M+)
- part on top of this just learns weight vectors... only ~1500 additional params

## BiDAF vs. BERT models
- BERT has way more params (110-330M versus 2.5M)
- BERT has no recurrence architecture; easier to parallelize
- BERT is pre-trained, BiDAF is only built on top of GloVe
	- pre-training is clearly a game changer, but it's expensive!
- ... are they really fundamentally different? probably not
	- BiDAF model interactions between questions and passages
	- BERT does self-attention across the entirety of its input, which is a concat of questions+passages

## Better pre-training objectives?
- probably yes
- traditional pre-training just masks out single words, but QA is a lot about *spans*
	- span-masking performs better than traditional masking on QA (SpanBERT)

## Is reading comprehension solved?
- reached superhuman performance, but definitely not solved
- current systems perform poorly on adversarial examples
- also hard to transfer systems to other datasets than the ones they were fine-tuned on

# Open-domain question answering
- not doing reading comprehension, instead have someone ask a question and we'll give an answer
	- assuming there's some large collection of documents like Wikipedia where we can retrieve answers

## Retriever-reading framework
- retrieve a document likely to be able to answer the question, then do reading comprehension task on that document
	- can use neural systems to train the reader as well! (Dense Passage Retrieval paper)
- **input**: a large collection of documents $\mathcal{D}$
- **output**: an answer string $A$

## Dense retrieval + generative models
- beneficial to generate answers instead of extracting them! --> T5
- **Fusion-in Decoder (FID)**: DPR + T5