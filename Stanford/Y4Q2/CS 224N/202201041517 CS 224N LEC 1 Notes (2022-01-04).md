#Stanford #Stanford-Winter-2022 #CS #CS224N #NLP #deep-learning 
___
# Introduction and Word Vectors
## Agenda
1. The course
2. Human language and word meaning
3. Word2vec introduction
4. Word2vec objective function gradients
5. Optimization basics
6. Looking at word vectors

*Key*: word meaning can be represented rather well by a high-dimensional vector of real numbers
## Logistics
- All important course info on course webpage
- First assignment out now, due on Tuesday at 3:15pm
- `python` /`numpy` / `pytorch`
	- Review session 1:30pm first two Fridays

## Learning Goals
1. Foundations of effective modern methods in NLP
	- word vectors, feed-forward networks, recurrent networks, attention, encoder-decoder models, transformers, etc.
2. Big picture understanding of human languages and difficulties of reproducing w/ computers
3. Practical ability to build systems (in PyTorch) for major NLP problems
	- Word meaning, dependency parsing, machine translation, etc.

### High-Level Goals of Assignments
Take people from 0 to state-of-the-art with deep networks for NLP
- all completed individually

# Human language and word meaning
Human language: primarily to do w/ people and what *they* mean (not just the words...)
- not much evidence at all that humans are much more intelligent than chimpanzees
	- chimps can plan, use tools, remember things (short-term memory)
	- except for language
- think of language as the networking system for human beings, much like computer networks
	- writing started ~5,000 years ago in China and the Middle East, independently

## Applications
**Neural Machine Translation**: today, quite good!
**GPT-3**: a first step on the path to foundation models

## How to represent the meaning of a word?
Commonest linguistic way of thinking of meaning:
- signifier (symbol) $\iff$ signified (idea or thing)
	- = *denotational semantics*
		- a way that meaning is thought of in programming languages
	- tree $\iff$ { ðŸŒ², ðŸŒ´, ðŸŒ³, ... }

### Usable meaning in a Computer
- **previously commonest NLP solution**: e.g. WordNet, thesaurus containing lists of *synonym* and *hypernym* sets (basically *is a* relationships)
	- e.g. `nltk` (an old utility for basic NLP tools, like a pocket knife)
	- problems w/ resources like WordNet
		- missing nuance: e.g. "proficient" listed as a synonym for "good"
		- also lists offensive synonyms w/o coverage of the appropriate connotations
		- human-compiled, so not good at keeping up with new meanings of words
			- subjective, and requires human labor to create / adapt
		- but key: *can't be used to accurately compute word similarity*

### Words as Discrete Symbols
in traditional NLP, words are discrete symbols (*localist* representation)
- can be represented by *one-hot* vectors -> vector dimension = number of words in vocabulary (which is huge...)
	- really the space of words is infinite if vocabulary is compositional...

*problem*: no notion of word similarity
- "hotel" and "motel" are very similar but not synonyms or hypernyms
	- one-hot vectors of the two will be orthogonal
	- WordNet's list of synonyms badly incomplete for this similarity task
- *solution*: learn to encode similarity in the vectors themselves

### Representing words by their context
**Distributional semantics**: a word's meaning is given by the words that frequently appear close-by
- "You shall know a word by the company it keeps" (Firth 1957)
- **context** of a word = set of words that appear nearby within a fixed-size window

#### Word Vectors
- similarity: **dot product**
- word meaning as a very high-dimensional vector space
	- can't show visualizations of high-dimensional vector-spaces, so project down to 2 dims
	- does a good job of relating the meaning of words in terms of other words

# Word2vec
- Mikolov et al. 2013
- a framework for learning word vectors
- idea:
	- long list of words (corpus of text)
	- every word in fixed vocab represented by a vector
	- go through each position $t$ in text, which has center word $c$, and context words $o$
	- use *similarity of word vectors* for $c$ and $o$ to calculate *probability of $o$ given $c$*
	- keep adjusting word vectors to maximize this probability
- e.g. we compute $P(w_{t+j} | w_t)$ (center word at position $t$, and outside context words $w_{t+1}$, $w_{t-1}$, ...)
- objective function
	- likelihood:
$$L(\theta) = \prod_{t=1}^T\prod_{-m\leq j\leq m; j\neq 0}P(w_{t+j} | w_t; \theta)$$
	- standard objective function $J(\theta)$ is average negative log-likelihood:
		- all of the products turn into sums, which are a lot easier to wrangle computationally (plus no underflow)
$$J(\theta) = -\frac{1}{T}\log L(\theta) = -\frac{1}{T}\sum_{t=1}^T\sum_{-m\leq j\leq m; j\neq 0}\log P(w_{t+j} | w_t; \theta)$$
	- $m$ is the window size and $T$ is the number of target words
	- how to calculate $P(w_{t+j} | w_t; \theta)$?
		- use *two vector representations* per word $w$:
			- $v_w$ when $w$ is center word, $u_w$ when $w$ is a context word
			- just makes math easier :)
		- then for center word $c$ and context word $o$:
$$P(o|c) = \frac{\exp(u_o^Tv_c)}{\sum_{w\in V}\exp(u_w^Tv_c)}$$
			- work out similarity btwn center word and context word
			- then normalize over entire vocabulary to give a probability distribution
			- $\exp$ makes everything positive
			- an example of the **softmax** function $\mathbb{R}^n\rightarrow (0, 1)^n$
				- good arbitrary way to turn real numbers into probabilities
				- "max" because it amplifies probability of largest $x$
				- "soft" because it still assigns some probability to smaller $x$

## Model training
$\theta$ represents *all* model parameters -- very high-dimensional vector
- with $d$-dimensional vectors and $V$-many words, we have $\theta\in\mathbb{R}^{2dV}$ (b/c every word has two vectors)
- optimize parameters in $\theta$ by walking down the gradient
	- "figure out what direction is down" i.e. use calculus
	- basic Lego piece: the chain rule
$$\frac{\partial x^T a}{\partial x} = \frac{\partial a^T x}{\partial x} = a$$
## Live demonstration in lecture
- reviewed partial derivative of $J(\theta)$ with respect to center word $v_c$
	- on Assignment 2 we'll do this for the context words $u_o$

$$\frac{\partial}{\partial v_c} \log\frac{\exp(u_o^Tv_c)}{\sum_{w=1}^V\exp(u_w^Tv_c)}$$
$$=\frac{\partial}{\partial v_c}\log\exp(u_o^T v_c) - \frac{\partial}{\partial v_c}\log\sum_{w=1}^V\exp(u_w^Tv_c)$$
- first partial is easy, the second one is harder (see material posted on website)

$$\frac{\partial}{\partial v_c} \log p(o|c) = u_o - \sum_{x=1}^V p(x|c)u_x$$
- the sum portion is an *expectation*: average over all context vectors weighted by their probability
- = observed - expected
	- want to make what we expected align with observed, i.e. make this quantity as small as possible
- just the derivatives for the center vector parameters -- also need derivatives for outside vector parameters, then have derivative w.r.t. all params and can minimize

## Gradient Descent
Algorithm used to minimize objective function: calculate gradient of $J(\theta)$ then take *small step in direction of negative gradient*, then repeat
- update equation: $\theta^{new} = \theta^{old} - \alpha\nabla_\theta J(\theta)$
- the full gradient $\nabla_\theta J(\theta)$ is very expensive to compute
	- use **stochastic gradient descent**: repeatedly sample windows and update after each window