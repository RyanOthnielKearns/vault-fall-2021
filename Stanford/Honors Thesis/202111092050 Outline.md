Ryan Othniel Kearns
Advised by Prof. Thomas Icard
Stanford University Dept. of Philosophy
November 9, 2021
# Proposed Thesis Outline
## 1. Trust as an End in Explainable Artificial Intelligence
> Cooperation between agents, in this case algorithms and humans, depends on trust. If humans are to accept algorithmic prescriptions, they need to trust them.

This is a quote from the Wikipedia entry for "Explainable artificial intelligence." Plenty of researchers in the field of Explainable AI take *trust* to be a primary end goal for their research. On an initial impression I agree -- as AI systems become increasingly complex and capable of superhuman tasks, it seems paramount that we learn to develop and evaluate trusting relationships with these systems. Trust is the dominant attitude determining the extent, productivity, and quality of our interactions with artificially intelligent systems. Moreover, a future in which trust is absent in human-machine relationships sounds increasingly dim as machines come to govern more and more of the important decisions in our lives, especially outside of our control.

However, the operative terminology in Explainable AI (henceforth XAI) does not include trust, but instead concepts like explainability, interpretability, fairness, and robustness. Instead of reaching for our goal of trust directly, XAI researchers take assurances of these other four concepts as (strongly) *entailing* or (weakly) *prompting* trust. Particularly the first two concepts, explainability and interpretability, are taken to be diagnostic criteria or "intermediate goals" for trust ([[202111092144 Filip Karlo Dosilovic et al. 2018, "Explainable Artificial Intelligence- A Survey" - Reading Notes|Dosilovic et al., 2018]]). Researchers like [[Ribiero et al. 2016, "'Why Should I Trust You?' Explaning the Predictions of Any Classifier" - Reading Notes|Ribiero et al., 2016]] have directly linked explainability to trust:
> Whether humans are directly using machine learning classifiers as tools, or are deploying models within other products, a vital concern remains: *if the users do not trust a model or a prediction, they will not use it*. (1)
> 
> Emphasis in the original

Moreover, other researchers like [[202111092149 Pearl Pu and Li Chen 2006, "Trust Building with Explanation Interfaces" - Reading Notes|Pu and Chen, 2006]], [[202111092148 Been Kim 2015, "Interactive and Interpretable Machine Learning Models for Human Machine Collaboration" - Reading Notes|Kim, 2015]], and [[202111092150 Zachary C. Lipton 2018, "The Mythos of Model Interpretability" - Reading Notes|Lipton, 2018]] have directly tied transparency to trust:
> Communication with the user is essential to a successful interactive system - it improves *transparency*, which has been identified as a major factor in establishing user *trust* in adaptive agents...
> 
> Kim 2015, p. 102 (emphasis mine)

The argumentative goal of this section will be just to establish the current status of the literature.

## 2. The Need for a Conceptual Analysis of Trust in XAI
We've established trust as an extant goal for XAI, but what precisely do we *mean* by trust? XAI researchers admit themselves that trust does not yet admit acceptable criteria for their purposes ([[202111092144 Filip Karlo Dosilovic et al. 2018, "Explainable Artificial Intelligence- A Survey" - Reading Notes|Dosilovic et al., 2018]]). I consider it the role of philosophical conceptual analysis to fill this gap.

In particular, I propose to investigate the four-place predicate $T(A, B, x, z)$, which says that "$A$ trusts $B$ with (or to do) $x$ in context $z$." For which kinds of quadruples can we say that $T$ holds? More specifically, what necessary and sufficient conditions must be in place for instances of $T$ to hold? In particular, I'll consider the cases where $A$ is a human with varying expert knowledge in AI (or related domains) and $B$ is an artificially intelligent system or composition of such systems.

The philosophical literature has a plurality of definitions for *trust* that I will not be able to cover exhaustively. However, for these specific purposes, it is important to note the *coherence* that such a notion of trust must enjoy with the related concepts of *explainability* and *transparency* in the XAI literature. Since explainability and transparency seem so tightly coupled with trust in XAI, a proper definition of the latter would not ignore them.

As almost an aside, here we can also raise the proposal that trust *reduces* to a combination of explainability, transparency, robustness, and other XAI-related notions. I can draw up some simple thought experiments to refute this so it's put out of the way before proceeding.

The argumentative goal of this section will be to set the stage for the aforementioned conceptual analysis and assert why I think it will be useful for XAI practitioners.

## 3. Previous Conceptual Work on Trust
This section will overview various philosophical projects to formalize trust and assess their relevance to the task concerning trust-in-XAI.

First, there's the mainstream philosophical take on trust. According to [[McLeod 2015, "Trust" - Reading Notes|McLeod 2015]] in his SEP article on "Trust," the predominant philosophical work thus far has considered *interpersonal* trust, beginning with the paradigmatic paper [[Annette Baier 1986, "Trust and Antitrust" - Reading Notes|Baier, 1986]] and continuing with [[Hardwig 1991, "The Role of Trust in Knowledge" - Reading Notes|Hardwig, 1991]], [[Holton 1994, “Deciding to Trust, Coming to Believe” (annotated).pdf|Holton, 1994]], and the like. This doesn't really help us, since we are particularly interested in relationships of trust between humans and artificial agents. Yet, I consider it worthwhile to include as it's cited in pretty much all of the following philosophical work.

After Baier and her contemporaries, there are more tailored but still generally philosophical accounts on trust. [[Nguyen 2021, "Trust as an Unquestioning Attitude" - Reading Notes|Nguyen, 2021]] proposes the idea of trust as an "unquestioning attitude." Then, more explicitly to the point of interactions with artificial agents, there is work like [[Taddeo 2010, "Modelling Trust in Artificial Agents, A First Step Toward the Analysis of e-Trust" - Reading Notes|Taddeo, 2010]] on "e-trust" between artificial agents, or [[Buechner and Tavani 2011, "Trust and multi-agent systems- applying the 'diffuse, default approach' to experiments involving artificial agents" - Reading Notes|Buechner and Tavani, 2011]] on the "diffuse, default model" for trust (drawing from [[Walker 2006|Walker, 2006]]). These models relax the interpersonal constraints present in Baier, Hardwig, and Holton's work, and allow trust to exist in a more general class of relationships, which can include humans and artificially intelligent systems.

The bulk of this section will be spent in particular reviewing sources like those in the paragraph above. Specifically, I want to identify the maximal set of properties all of these modern theories take to be characteristic of trust. These properties can serve as a foundation should I later further my own account of trust for XAI, which I will attempt in [[202111092050 Outline#6 Various Potential Proposals including Reformalized Objectives for XAI|Section 6]].

## 4. A Puzzle to Address
In this section, I introduce a puzzle concerning the relationship between trust and explainability / transparency / interpretability. Suppose we take theories like Nguyen's and Taddeo's to be at least initially acceptable in characterizing trust in XAI. In fact, there does not seem to be much work I can find suggesting anything else. So if we run with these theories, we have to contend with the fact that they define trust and transparency as definitionally *opposed*. Transparency is the supervision of inner mechanisms and decision procedures, and (under these definitions) trust is *exactly* the choice of refraining from supervising in this way. So, how can transparency entail or prompt trust in the way we want if this tension exists?

Nguyen has another paper from 2021 I will employ in this section called [[202110102255 Nguyen 2021, "Transparency is Surveillance" - Reading Notes|"Transparency is Surveillance."]] His arguments for pitting transparency against trust are mostly restricted to the setting of domain-expert human groups, like climate scientists, but I think there are general enough points there to levy towards the tension I mentioned above.

In this section I will argue that such a tension first *exists*, and then that it runs against our goal of forming a coherent conceptual picture of the means and ends in XAI. So, we must tighten our definitions of these various concepts to do away with the problem.

## 5. Descriptive Accounts of Trust in Computer Systems
A first place to look for a solution to our problem is the analysis of trust in computer systems more broadly. Of course, computer scientists have *something* they intend to say when they say "explanation (or transparency) entails trust." What can we glean from their own accounts and experiments that might better codify what trust is for XAI? This section involves reading and analyzing papers like [[202111092256 Andrea Papenmeier et al. 2019, "How model accuracy and explanation fidelity influence user trust in AI" - Reading Notes|Papenmeier et al., 2019]], in which the authors conduct an empirical survey to track how users' self-reported trust was influenced by both model accuracy and explanations. Related are further empirical works by [[202111092149 Pearl Pu and Li Chen 2006, "Trust Building with Explanation Interfaces" - Reading Notes|Pu and Chen, 2006]] and [[202111092259 Alyssa Glass et al. 2008, "Toward Establishing Trust in Adaptive Agents" - Reading Notes|Glass et al., 2008]], which conduct further experiments on both layperson and domain-expert users. While not conceptual analysis themselves, such papers propose operational definitions for trust and related notions that will be insightful.

This section is the least developed in this outline so far, and I'm not quite sure what I will find from a thorough analysis of the papers mentioned above and related ones! Given that more recent papers (e.g., survey papers like [[202111092144 Filip Karlo Dosilovic et al. 2018, "Explainable Artificial Intelligence- A Survey" - Reading Notes|Dosilovic et al., 2018]]) have balked at the lack of formal criteria for trust in XAI, I am guessing that these empirical approaches yield nice results for particular software subdomains but do not yet provide compelling answers to the questions we're asking here. Nonetheless, they may be useful.

## 6. Various Potential Proposals including Reformalized Objectives for XAI
In the final substantive portion of the thesis, I have a few argumentative approaches I may try out. For one, I think one thing missing from the discussion of transparency, explainability, and trust thus far is the temporal and narrative structure in which they interact. Given a particular user and a particular system, at a static moment in time, transparency and trust seem opposed. Yet, when we consider the narrative of the relation between user and system, it is possible that transparency (or explainability) play certain causal roles in the formation of a trusting relationship that manage to avoid the opposition. For example, making system decision criteria transparent or explainable in the right way early on might promote the user's trust in the system, *insofar as* the user is disposed not to doubt the system's decrees at some later time step. This way, the user is never trusting and demanding transparency simultaneously, but rather inspecting now, coming to trust, and then trusting later.

It also seems possible, given the outcome of the previous argument, that the end goal of XAI is *not trust on its own*, but rather some context-sensitive expectancy of positive outcome that can be fulfilled by either explanation, transparency, or trust, depending on circumstance (to explain that "context-sensitive expectancy of positive outcome" itself is not just trust will actually also take some work). If all the user wants to know is that the system will cooperate in some particular way (which is to be defined), they can either trust it, or do away with trust and conduct a thorough audit of the system to their liking. In situations when both options are available it actually seems *wrong* to still say that trust is the goal -- would auditing not achieve the thing you *really* want in that case? It could be that *trust* in AI systems is relegated to the particularly sensitive situations in which explanations and transparency are not available -- say, a proprietary algorithm assigning operating rooms in a triage scenario.

This will be the most philosophically weighted portion of the thesis, and an outline won't do it justice. I'm still working on the particularities of the two arguments above, and informing them also by what I find in [[202111092050 Outline#5 Descriptive Accounts of Trust in Computer Systems|Section 5]], which is also in an early stage.