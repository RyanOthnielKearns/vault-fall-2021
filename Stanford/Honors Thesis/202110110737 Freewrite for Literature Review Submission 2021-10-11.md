In our conversation last Tuesday, October 5, we touched on a number of questions related to different aspects of the thesis. The first bundle of questions has to do with fixing the scope of our consideration of trust within Explainable AI, and the latter launches more specific critiques on existing theories of trust and their candidacy for explaining the phenomenon under discussion in Explainable AI.

First, at a higher level, we addressed the discrepancy between "being trusted" and "being trustworthy." There is the question of whether "trustworthiness" as a predicate even needs mentioning when it comes to the issue of trusting AI. Trustworthiness is attractive as it assigns a property to the object "up for trusting," which allows us to more concisely talk about trust in relation to some object -- otherwise, we would need to spell out the individual instances of trusting that apply that make for trustworthiness. However, we also talked about trust at this "instance level" being a four-place predicate: $A$ trusts $B$ with (or to do) $x$ in context $c$. Specifically within XAI, $A$, $x$, and $c$ seem to admit so much variability that a discussion of the trustworthiness of $B$ seems like it would have to engage with individual instances of trusting to be useful, since not a whole lot would generalize. For example, for $B$ to be trustworthy, full stop, we'd need to cover cases where $A$ is both domain expert and layperson, which are already very different cases. Thus, I think the primary phenomena under consideration is $Trusts(A, B, x, c)$ and not $Trustworthy(B)$. So when XAI abstracts say something like "explaining our model gives us reasons to trust it," variables $A$, $x$, and $c$ are most often left implicit or contextual, but it seems like they persist both on the explaining and the trusting side of the equation. We'll need to get specific about the intended values for these variables, of course -- is it a goal for XAI that $A$ be unbounded, e.g. that any common person should be able to receive an explanation? Or are certain explanations only available for very highly specialized domain experts like radiologists or court judges?

Another high level discussion point we addressed was the tension between system complexity and accuracy. My initial reaction was that within conversations around AI Explainability, the relationship between complexity and accuracy seems like straightforward tension. This is not always true -- in certain domains simple systems like decision trees can yield all the accuracy you might want, and applying deep neural networks to the problem is introducing complexity for no gain. Also, overfitting is a phenomenon affecting all domains of machine prediction tasks, and indicates a point where initial complexity actually hinders accuracy. There is somethign to the point, though, that complexity and accuracy are at odds. Doshi-Velez et al., in their 2017 paper "The Role of Explanation in Algorithmic Trust," cite one often-heard complaint about AI Explanation that it may degrade system performance by incentivizing algorithm designers to persue simpler yet more explainable architectures. This complaint needs to be addressed by AI Explainability in general -- I don't think it holds water, because not only are complexity and accuracy nontrivially related, but complexity and explainability itself are not directly at odds either. Understanding the interplay of these terms will be important for any sensible proposal I might come up with.

Now getting more into the weeds with particular trust theories, a related tension we discussed was that between explaining or interpreting and trusting. This is a particular hurdle for adopting Nguyen's view, which says trust is a disposition to not question the trustee's actions and motivations. It seems like explaining or interpreting is in fact doing the exact opposite -- actively questioning the trustee's abilities and decision-making structure. Does this mean we come to trust algorithms *less* when we explain them? This cannot be totally right. Nguyen actually has something to say on this in a more recent preprint I stumbled upon, having to do with the irreconcilable tension between trust and transparency (Nguyen 2021, "Transparency is Surveillance"). I don't think that this latter paper addresses the objection outright, because the transparency Nguyen considers seems mostly to do with intrusion into communities of experts, not inanimate decision-makers like machine learning algorithms. Nontheless, it will be helpful to consider these two papers in concert. *Transparency* is yet another ill-defined word tossed about freely in Explainable AI. The Forbes article I cited for my prospectus actually says that transparency ensures trust, in total contradiction to Nguyen's argument (see Kearns 2). So, part of a proper treatment of trust will involve contextualizing not just explainability but transparency as well as a separate aim with complex relational quality.

Finally, we talked about the possibility of beginning with a more naive calculus, like Bhattacharya's model of trust as expectation of positive outcome. I think such models, drawn predominantly from economics, can be helpful because they aren't baked into interpersonal accounts the way philosophical theories are, which can make them easier to disentangle from their original intended usages. Taddeo's model of e-trust is another model that operationalizes expected value, though like Bhattacharya's, it is restricted to rational decision-makers as subjects (Taddeo 2010). There is something to this, though -- it represents an ideal use case we can build upon if we want to consider less rational and more normatively loaded cases, like humans trusting robot collaborators.