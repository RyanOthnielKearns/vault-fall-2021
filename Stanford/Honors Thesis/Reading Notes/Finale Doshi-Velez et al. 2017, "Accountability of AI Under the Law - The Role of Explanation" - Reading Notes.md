# [[Finale Doshi-Velez et al. 2017, “Accountability of AI Under the Law - The Role of Explanation” (ANNOTATED).pdf|Doshi-Velez et al. 2017, "Accountability of AI Under the Law: The Role of Explanation"]]
# #reading-notes 
Tags: #explainable-ai #explanation #accountability #ai-law
___
This paper comes from a conference of scholars at the [Berkman Klein Center Working Group on AI Interpretability](https://cyber.harvard.edu/publications/2017/11/AIExplanation) at Harvard. They recognize that due to two challenges -- (1) that AI routinely make common-sense mistakes in decision-making that humans would avoid, and (2) that humans decide objective functions for AI, introducing an additional dimension of error into the system -- there is a need to discuss AI explainability as a legal mandate in more detail.

The authors define #accountability as "the ability to determine whether a decision was made in accordance with procedural and substantive standards and to hold someone responsible if those standards are not met" (2). Those "procedural and substantive standards" allude to legal mandate, and the authors bring up a number of examples of legally required explanations for humans and corporations. As one central thesis, the authors suggest that we can start legal requirements for AI explanations by asking of them what we ask of people. Importantly, the authors recognize the caveat that AI must be required to provide the explanation *ex ante* -- while humans can conjure post-hoc rationalizations that suffice for explanations under the law, AI cannot generate explanations (at least, neither #local-explanations nor #counterfactual-explanations) without knowing the human-interpretable components it will be expected to give before the fact. The authors also balance critiques that requiring explanations from AI might hinder innovation, specifically because storing data and building systems for explanation is costly, or expose trade secrets.

In terms of explanations, the authors describe a form of #local-explanations largely inspired by [[Ribeiro et al., 2016|Ribeiro et al., 2016]] and others, as well as #counterfactual-explanations drawn mostly from [[Wachter et al. 2017, "Counterfactual Explanations Without Opening the Black Box" - Reading Notes|Wachter et al. 2017]] (p. 4-5). They also indicate that the desire for explanations changes depending on (1) the impact / significance of the decision, (2) the ability to contest a decision and assign blame, (3) our reasons to think an error occurred with the decision (p. 6-7).