#honors-thesis #philosophy #meeting-notes
# Prep
- coming in with two large areas of focus
	1. the conceptual analysis vs. engineering angle
		- the broad argumentative tack (e.g. advocating a concept of trust to apply when relationships aren't an apt contextual setting)
	2. approaching the drafting and revision / gradual improvement of a larger body of work like this
		- how to make or measure steady improvement / how to make the best use of philosophical mentors?

# Meeting Notes
- 20 pages seems pretty short...
	- could imagine a world where you write to 30 pages
- word count: standard journal article
	- absolute max: 10K (only like J Phil...)
	- because an undergrad thesis: expect to have some parts better than others -- hitting exactly 20 pages would likely shrink to 15 in terms of quality
- path between now and a first draft that's not good / polished / etc. but has the core of the argument you want to try making
	- *do that*! this week
	- much better to iterate early than to sit on it and come to readers later
- *reverse outline*
	- take the draft and produce an outline that describes what the draft says (e.g. by collecting all of the topic sentences)
	- go to potential readers with *both* the complete draft and the reverse outline
	- looking for big picture argumentative stuff at this time
- details of the plan make a difference to how you pitch the thesis
	- if goal is philosophy grad school, wouldn't bother getting a computer science professor as a second reader
	- maybe goal is submission to FACT or AIES, audience will be interdisciplinary AI/ML people, etc. -> very different story
	- have to locate your placement on that spectrum
		- less about what am I citing, more about who's questions am I responding to
- "we want to provide a conceptual framework for when explainability does implement trust"
- if drawing heavily from Nguyen, should be part of the framing
- $T$ works with more than one theory
	- not using it to abjudicate btwn theories, so what is it doing? -> making clear / logically precise sentences of the form "this explanation helped me to trust this algorithm" or whatever
		- not a problem that two individuals in a lab might disagree on whether to trust LIME given some set of explanations -- we need some basis of shared assumptions, otherwise we could just reduce to the setting of taking psychological polls to tell whether something counts as trustworthy