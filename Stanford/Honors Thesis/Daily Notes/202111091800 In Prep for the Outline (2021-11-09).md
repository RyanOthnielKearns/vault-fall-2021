#daily-notes #outline
___
In prep for the outline, what are the facts?

Literature review resources might be helpful:
- [[202110102213 Literature Review 2021-10-11]]
- [[202110051118 Proposed Literature Review List]]
- [[202110110737 Freewrite for Literature Review Submission 2021-10-11]]
- [[Thesis_Freewrite_and_Initial_Literature_Review.pdf]]

1. Lots of XAI papers take for granted that one of the end goals of explainable AI is *trustworthy* AI (notably [[Ribiero et al. 2016, "'Why Should I Trust You?' Explaning the Predictions of Any Classifier" - Reading Notes|Ribiero et al. 2016]], ==look for others==)
2. In particular, there are assertions that we *reach* trustworthiness through various virtues in explainable AI:
	1. *explainability* ([[Ribiero et al. 2016, "'Why Should I Trust You?' Explaning the Predictions of Any Classifier" - Reading Notes|Ribiero et al. 2016]], ==wanting to find others==)
	2. *transparency* ([[Madhvi Mavadiya 2020, "Don't Blame the Algorithm, Trust It" - Reading Notes|Mavadiya 2020 (Forbes)]], [[Zachary C. Lipton 2018, "The Mythos of Model Interpretability".pdf|Lipton 2018]] p. 21, apparently a section covered in [[2021 Sep 15 Daily Notes#Unstructured reading notes|my notes from Sep 15]])
		1. [[Been Kim 2015, "Interactive and Interpretable Machine Learning Models for Human Machine Collaboration".pdf|Kim 2015, p. 17, 102]]
		2. [[Pearl Pu and Li Chen 2006, "Trust Building with Explanation Interfaces".pdf|Pu and Chen 2006]] (which I haven't read)
		3. [[Alyssa Glass et al. 2008, "Toward Establishing Trust in Adaptive Agents".pdf|Glass et al. 2008]] (which I haven't read)
	3. *interpretability* ([[Zachary C. Lipton 2018, "The Mythos of Model Interpretability".pdf|Lipton 2018]], p. 4) (this may just be transparency though)
3. But what *is* trustworthiness, restricted to this XAI scope? In particular, what constitute the necessary and sufficient conditions required for us to attribute *trustworthiness* to some artificial intelligence agent $A$?
	1. Such conditions should be sensitive to the fact that explanation and interpretability seem to be tightly related to trust in the XAI context
4. Where can we look for foundational work on trust that might help answer this question?
5. For one, there's the mainstream philosophical take on trust. According to [[McLeod 2015, "Trust" - Reading Notes|McLeod 2015]], the predominant philosophical work on trust has considered *interpersonal* trust, beginning with the paradigmatic paper [[Annette Baier 1986, "Trust and Antitrust" - Reading Notes|Baier, 1986]] and continuing with [[Hardwig 1991, "The Role of Trust in Knowledge" - Reading Notes|Hardwig 1991]], [[Holton 1994, “Deciding to Trust, Coming to Believe” (annotated).pdf|Holton 1994]], etc. This doesn't really help me, since I'm particularly interested in relationships of trust between humans and artificial agents.
6. After this there are more contemporary but still generally philosophical accounts on trust. [[Nguyen 2021, "Trust as an Unquestioning Attitude" - Reading Notes|Nguyen 2021]] proposes the idea of trust as an "unquestioning attitude." Then, more explicitly to the point of interactions with artificial agents, there is work like [[Taddeo 2010, "Modelling Trust in Artificial Agents, A First Step Toward the Analysis of e-Trust" - Reading Notes|Taddeo, 2010]] on "e-trust" between artificial agents, [[Buechner and Tavani 2011, "Trust and multi-agent systems- applying the 'diffuse, default approach' to experiments involving artificial agents" - Reading Notes|Buechner and Tavani]] on the "diffuse, default model" for trust (drawing from [[Walker 2006]])
7. But there's a puzzle we need to address: many authors have thought (quite naturally) that trust and transparency (or likewise trust and explanation) are *opposed notions*. If they're opposed, how are we to accept a thesis that transparency or explanation *engender* trust?
	1. E.g. [[Taddeo 2010, "Modelling Trust in Artificial Agents, A First Step Toward the Analysis of e-Trust" - Reading Notes|Taddeo 2010]] says that the more an agent trusts, the less they will supervise or intervene in another agent's behavior
	2. [[202110102255 Nguyen 2021, "Transparency is Surveillance" - Reading Notes|Nguyen 2021 in "Transparency is Surveillance"]] has made the point that at least for expert human actors, transparency does not engender trust but instead can promote deception
8. So what gives?
	1. For one thing, we can look at how computer scientists theorize about this implication (explanation / transparency $\rightarrow$ trust) and see if it addresses the puzzle
		1. [[Andrea Papenmeier et al. 2019, "How model accuracy and explanation fidelity influence user trust in AI".pdf]]
		2. [[Alyssa Glass et al. 2008, "Toward Establishing Trust in Adaptive Agents".pdf]]
		3. [[Pearl Pu and Li Chen 2006, "Trust Building with Explanation Interfaces".pdf]]
	2. For another, we can reformalize trust philosophically to make way for it's cohabitation with explanation and transparency in XAI
		1. Specifically, I think this requires the argument that transparency, explanation, and trust play temporal roles in the story of a user and AI's interaction, and that transparency can lead to trust *when it's revoked later* which is an interesting kind of cause and effect
		2. In particular, cases where transparency remains throughout the duration of a user's involvement with a system *are not* cases of trust between the user and that system, since the trust "role" (get more specific on this) is being fulfilled by the availability of transparency
		3. So, what we're *really* looking for it *not* trust, but some sort of assurance that the AI system can be safely cooperated with -- we want something like an expectation of positive outcome, and we achieve that through a combination of transparency, explanation, and trust (and maybe other things like fairness and robustness guarantees)