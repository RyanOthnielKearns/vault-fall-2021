#CS #CS224W #lecture-notes #deep-learning #GNNs #graphs
# Agenda
## Project Proposal Logistics
- blog posts for graph neural networks applied to different domains
- like a tutorial incl. Colab for how it works
- use PyG, see examples & use cases

## Deep Learning Methods with Graph Neural Networks
$ENC(v) =$ multiple layers of deep network (non-linear transformations)
- graph neural networks subsume both CNNs and transformers
	- modern ML toolbox: designed for simple grids (images) or sequences (text)
- can combine with node similarity functions

1. Basics of deep learning
2. Deep learning for graphs
3. Graph Convolutional Networks
	- most basic model, generalization of convolutional neural networks
4. GNNs subsume CNNs and transformers

# Recap
## Node Embeddings
map nodes to $d$-dimensional embeddings such that similar nodes are embedded close together
- goal: $similarity(u, v) \approx z_u^\top z_v$ (decoder)
	- $ENC(v) = z_u$

### Limitations of Shallow Encoders
- $O(|V|)$ parameters needed
	- no sharing between nodes
- inherently "transductive" -- cannot generate embeddings for nodes not seen during training
- no incorporating node features

# Basics of Deep Learning
## Supervised learning
given input $x$, goal is to predict label $y$
- an optimization problem:
$$\min_\Theta\mathcal{L}(\mathbf{y}, f(\mathbf{x}))$$
- $\Theta$: set of parameters to optimize
- $\mathcal{L}$: loss function
	- e.g. L2 loss: $\mathcal{L}(\mathbf{y}, f(\mathbf{x}))=\Vert y-f(x)\Vert_2$
	- L1 loss, huber loss, max margin (hinge loss)
	- cross entropy loss
		- when predicting categorical label (either 0-1 or multiclass)
			- $\mathbf{y}$ is one-hot encoded
			- $f(\mathbf{x})=\text{Softmax}(g(\mathbf{x}))$
		- $CE(\mathbf{y}, f(\mathbf{x})) = -\sum_{i=1}^C(y_i\log f(x)_i)$

## Optimizing the Objective Function
$$\nabla_\Theta\mathcal{L} = \bigg(\frac{\partial\mathcal{L}}{\partial\Theta_1}, \frac{\partial\mathcal{L}}{\partial\Theta_2}, \dots\bigg)$$
- gradient: directional derivative in the direction of largest increase
- iterative algorithm:
$$\Theta\gets\Theta - \eta\nabla_\Theta\mathcal{L}$$
	- $\eta$: learning rate (hyperparameter)
- ideal termination condition: gradient = 0
	- in practice: stop training when improvement on the validation set stops
### Stochastic Gradient Descent
- computing the exact gradient is hard: requires summing over the entire training data set
	- solution: at each training step pick a different minibatch $\mathcal{B}$ as a subsample of training set
		- **batch size**: number of data points in a minibatch
- **iteration**: 1 step of SGD on a minibatch
- **epoch**: one full pass over the dataset
- unbiased estimator of full gradient

## Neural Network Function
### Back-propagation
- if $\mathcal{L}$ is composed of some set of "building block" functions $g$, and have derivatives $g'$...
	- can automatically compute $\nabla_\Theta\mathcal{L}$ by evaluating each gradient $g'$ on minibatch $\mathcal{B}$
	- use **chain rule** to propagate gradients of intermediate steps:
$$f'(x) = g'(h(x))h'(x)$$

### Non-linearity
- Rectified linear unit (ReLU): $ReLU(x) = \max(x, 0)$
- Sigmoid: $\sigma(x) = \frac{1}{1+e^{-x}}$
#### Multi-layer Perceptron (MLP)
each layer combines linear transformation w/ non-linearity
$$\mathbf{x}^{(l+1)}=\sigma(W_l\mathbf{x}^{(l)}+b_l)$$

# Deep Learning for Graphs
1. Local network neighborhoods
	- describe aggregations, computation graphs
2. Stacking multiple layers

## Setup
- Graph $G$:
	- Vertex set $V$
	- Adjacency matrix $\mathbf{A}$ (assume binary)
	- Matrix of node features $\mathbf{X}\in\mathbb{R}^{m\times|V|}$
	- $N(v) =$ set of neighbors of node $v\in V$

## Challenges
- No fixed notion of "locality"
- No canonical node orderings -- solution must be **permutation invariant**
	- we want to learn function $f:G\rightarrow (\mathbf{A},\mathbf{X})$ s.t. $f(A_1, X_1)=f(A_2, X_2)$ where $A_i, X_i$ are different *order plans* of what is actually the same graph!
	- Also **permutation equivariance**: similarly for node representation for function $f : G\rightarrow\mathbb{R}^{m\times d}$
	- GNNs consist of permutation invariant / equivariant layers
		- *Not the case for other neural network architectures* e.g. MLPs

# Graph Convolutional Networks
*Main idea*: node's neighborhood defines a computation graph
- let input graph define the neural network architecture
- propagate info across graph to compute node features
	1. determine computation graph
	2. message passing

## Aggregate Neighbors
- generate node embeddings based on local network neighborhoods
	- nodes aggregate info from neighbors using neural networks
- many layers
	- layer 0 embedding of node $v$ is its input feature $x_v$
	- layer-k embedding gets info from nodes that are $k$ hops away
- basic approach: average information from neighbors and apply neural network transformation
- see slide titled "The Math: Deep Encoder"
- must be permutation equivariant

## How to train the GCN?
- model parameters: one $W_k$ and one $B_k$ per layer, but shared across all the nodes (again, see slides for math)
	- $W_k$: weight matrix for neighborhood aggregation
	- $B_k$: weight matrix for transforming hidden vector of self
	- every computation graph will be homogeneous
- can be performed efficiently via sparse matrix operations
	- matrix of hidden embeddings $H^{(k)}$
	- $H^{(k+1)} = D^{-1}AH^{(k)}$
		- $D^{-1}$ is inverse of the diagonal degree matrix (and also diagonal)
		- rewriting: $H^{(k+1)}=\sigma(\tilde{A}H^{(k)}W_k^\top + H^{(k)}B_k^\top)$
			- where $\tilde{A} = D^{-1}A$
				- sparse: so efficient sparse matrix multiplication can be used (not true for all GCNs, especially if aggregation functions are complex)

### Unsupervised training
- enforce "similar nodes have similar embeddings" using loss function (e.g. cross entropy between "similar" predicate & decoder, like inner product)
### Supervised training
- directly train (like logistic regression) using e.g. binary cross entropy loss

### Model design overview
1. Define neighborhood aggregation function
2. Define loss function on embeddings
3. Take set of nodes, define their comptuation graphs, and train

- **inductive capacity**: can generalize to unseen nodes (or even sections of graphs or entire graphs)
	- num model params is sublinear in $|V|$
	- train on one graph, generalize to a new graph
	- can also generate new node embeddings "on the fly" as graph evolves in real time