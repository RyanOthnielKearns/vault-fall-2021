#CS #CS224W #lecture-notes #Stanford #Stanford-Fall-2021 
___
# Agenda
- viewing graphs as a matrix
	- relate to random walks and embeddings from [[202110061645 CS 224W Lecture 2 Notes (2021-09-23)|past lecture]]
- node embeddings via **matrix factorization**
- random walk <> matrix factorization <> node embeddings closely related!
- **Link analysis approaches** for computing importance of nodes in a graph
	- PageRank
	- Personalized PageRank
	- Random Walk with Restarts

# PageRank (the Google Algorithm)
## The Web as a Graph
- nodes = web pages
	- what about dynamic pages created on the fly, or "dark matter" like database generated pages?
		- huge amount of the web locked in via logins / passwords
- edges = hyperlinks
	- think early days of the web -- hyperlinks were primarily for navigation not transaction
- represent as directed graph!
- *insight*: not all web pages are equally important -- **rank** web pages using the web graph link structure!
	- A form of **link analysis**

## Algorithm (Flow Model)
- links as "votes" -- page is more important if it has more links
	- lots of in-links = trustworthy or important
	- are all votes equal? -> links from important pages should count more (*recursive* definition!)
	- if page $i$ has importance $r_i$ and $d_i$ out-links, each out-link gets $r_i / d_i$ votes
	- page $j$'s importance $r_j$ is the sume of votes of its in-links

$$r_j = \sum_{i\rightarrow j}r_i / d_i$$
- Can you solve as a system of equations via Gaussian elimination?
	- does not scale to actual size of the web graph... something like cubic algorithm

## Matrix Formulation
- equivalent to flow model
- stochastic adjacency matrix $M$
	- page $j$ has $d_j$ out-links
	- if $j\rightarrow i$, then $M_{ij}=1/d_j$
	- **column stochastic** because columns sum to 1
- rank vector $r_i$ is importance score of page $i$
	- $\sum_i r_i = 1$
- *then*:

$$r = M\cdot r$$

## Connection to Random Walk
imagine random web surfer, following out-links uniformly at random
- $p(t)=$ vector whose $i$th coordinate i the prob that surfer is at page $i$ at time $t$
	- prob. dist. over pages
	- $p(t+1)=M\cdot p(t)$ (rewritten in matrix form!)
- suppose random walk reaches steady state $p(t+1) = M\cdot p(t) = p(t)$
	- $p(t)$ is a **stationary distribution** for the random walk (same for $r$ in matrix form where $r=M\cdot r$)
- PageRank = limiting distribution of random walk

## Connection with Eigenvectors of Matrices
- $A\in\{0,1\}^{n\times n}$
- vectors satisfying: $\lambda c=Ac$
	- $c$: eigenvector
	- $\lambda$: eigenvalue
- this is the definition of **[[202110061645 CS 224W Lecture 2 Notes (2021-09-23)#Tasks|eigenvector centrality]]**
	- flow equation: $1\cdot r = M\cdot r$
	- so: rank vector $r$ is a (principal) eigenvector of the stochastic adj. matrix $M$ (with eigenvalue $\lambda = 1$)
- these 4 formulations are all equivalent!
	- efficiently solve for $r$ using method called **Power Iteration** -- keep multiplying a randomly initialized vector by $M$, vector will converge to satisfy $r$

## Power Iteration
- use iterative procedure
	- each node gets initial page rank
	- repeat to converge:

$$r_j^{(t+1)}=\sum_{i\rightarrow j}r_i^{(t)} / d_i$$

- $r^{(t+1)}=M\cdot r^{(t)}$
- stop when $|r^{(t+1)}-r^{(t)}|_1<\varepsilon$
	- about 50 iterations in practice

### Why does this converge?
- does this converge?
- to what we want?
- are the results meaningful / reasonable?

#### Problems
1. some pages are **dead ends** (no out-links)
	- importance "leaks out" (mathematical problem -- random walker "drops" and value of random vector becomes 0)
	- this affects the column stochastity of the matrix $M$ if a node has no out-going edges
		- jumping at random gives each row value have $1/N$, meaning matrix is once again column stochastic
2. spider traps (all out-links within a group)
	- this group eventually absorbs all the importance (doesn't break column stochastic condition, but doesn't give us what we want)
	- **solution**: introduce random jumps
		- with prob $\beta$, follow link at random (typically $0.8\leq\beta\leq 0.9$)
		- with prob. $1-\beta$, jump to random page
		- follow "teleportation" rule with total probability 1 from dead ends

## Formulation
$$r_j\sum_{i\rightarrow j}\beta(r_i / d_i)+(1-\beta)(1 / N)$$
- summation "doesn't sum over teleportation edges"

$$G = \beta M + (1-\beta)[1/N]_{N\times N}$$
- matrix formulation solved by Hadoop or MapReduce!

# Personalized (Topic-Specific) Page Rank / Random Walk with Restarts
- a way to compute proximity of nodes (ranks relative to *teleport set* $S$)
	- proximity on graphs: what is the most related item to item $Q$?
- idea: every node has importance that they "push" to their neighbors
- simulate a random walk with a given set of `QUERY_NODES`
	- with probability $\alpha$, restart the walk at one of the `QUERY_NODES`
	- record visit count of each node
	- gives notion of proximity to the query node set
		- very simple and cheap recommender system!
- "similarity" notion considers
	- multiple connections
	- multiple paths
	- edge direction
	- degree of the node
- "restarts" mean that random teleportation is not to any node, but either set of "starting" ("query") nodes or even a single root node

# Summary
- graph naturally represented as matrix
- random walk process over a graph = random surfer moving across links w/ teleportation
- PageRank: limiting distribution of the surfer location represents node importance
	- leading eigenvector of transformed adjacency matrix $M$

# Matrix Factorization and Node Embeddings
- on [[202110072334 CS 224W Lecture 3 Notes (2021-09-28)|tuesday]], talked about embedding matrix as "shallow encoder"
	- nodes $u, v$ are similar if connected by an edge
	- means $z_v^Tz_u = A_{u,v}$
	- means $Z^TZ = A$
		- exact factorization is generally not possible, but we can learn $Z$ approximately!
		- objective: $\min_Z ||A-Z^TZ||_2$
			- in Lecture 3 used softmax instead of L2 norm, but goal to approximate $A$ is the same
	- `DeepWalk` and `node2vec` have more complex notion of node similarity -> equivalent to much, much more complex matrix factorization expressions

## Limitations
1. cannot obtain embeddings for nodes not in training set
	- if new node appears, have to redo the entire embedding step
2. cannot capture structural similarity
	- structurally identical nodes far away will have very different embeddings, b/c it's very unlikely that random walks will reach between the two
3. cannot utilize node, edge, or graph features
	- feature vectors = node labels

graph neural networks address these limitations!